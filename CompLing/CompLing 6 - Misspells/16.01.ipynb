{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_miss(correct):\n",
    "    miss = {}\n",
    "    for i in range(len(correct)):\n",
    "        # print(correct[:i:] + correct[i+1::])\n",
    "        miss[correct[:i:] + correct[i+1::]] = correct\n",
    "    return miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'втобус': 'автобус', 'атобус': 'автобус', 'авобус': 'автобус', 'автбус': 'автобус', 'автоус': 'автобус', 'автобс': 'автобус', 'автобу': 'автобус'}\n"
     ]
    }
   ],
   "source": [
    "print(generate_miss(\"автобус\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct(wrong, misses):\n",
    "    for i in range(len(wrong)):\n",
    "        #print(wrong[:i:] + wrong[i+1::])\n",
    "        if wrong[:i:] + wrong[i+1::] in misses.keys():\n",
    "            return misses[wrong[:i:] + wrong[i+1::]]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrector(corrects: list, misspell):\n",
    "    misses = {}\n",
    "    for w in corrects:\n",
    "        misses.update(generate_miss(w))\n",
    "    #print(misses)\n",
    "    return get_correct(misspell, misses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'автобус'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrector([\"автобус\", \"трамвай\", \"троллейбус\"], \"автомус\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrector([\"автобс\", \"трамвай\", \"троллейбус\"], \"автомус\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.0\n",
      "    Uninstalling numpy-2.2.0:\n",
      "      Successfully uninstalled numpy-2.2.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.12/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11728 sha256=08d73508a42e432f68781651c7e5d98679e0d5885a78b8093ad4f3b27f6244c8\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: seaborn in /home/codespace/.local/lib/python3.12/site-packages (0.13.2)\n",
      "Collecting a\n",
      "  Downloading a-1.0.tar.gz (564 bytes)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/codespace/.local/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/codespace/.local/lib/python3.12/site-packages (from seaborn) (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Building wheels for collected packages: a\n",
      "  Building wheel for a (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for a: filename=a-1.0-py3-none-any.whl size=1178 sha256=8fe8e846df8260ba48238c89345b65daf7ee9cef5e18c38e01919712fd73aeff\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/92/fc/0c/ce3763f832e95b02ca3a2dc760be92f97b327582a744477f88\n",
      "Successfully built a\n",
      "Installing collected packages: a\n",
      "Successfully installed a-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install pymorphy3 --q\n",
    "!pip install wikipedia\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import gensim\n",
    "import urllib.request\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')  \n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "sw = stopwords.words('russian')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_w(text): # Приводит к н.ф., запись и возращает все токены текста\n",
    "    sents = sent_tokenize(text, language='russian')\n",
    "    cleaned_text = []\n",
    "    all_words = [] \n",
    "    for sen in sents:\n",
    "        cleaned_text.append([morph.parse(w.lower())[0].normal_form \\\n",
    "                            for w in tokenizer.tokenize(sen) \n",
    "                            if morph.parse(w.lower())[0].normal_form not in sw])\n",
    "    for s in cleaned_text:\n",
    "        for w in s:\n",
    "            all_words.append(w)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_data.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = [morph.parse(w.lower())[0].normal_form for w in tokenizer.tokenize(data) if morph.parse(w.lower())[0].normal_form not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/i-love-crispy-laptops/CompLing/CompLing 6 - Misspells/16.01.ipynb Ячейка 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bpotential-eureka-6qvg95vvgqjf4xq/workspaces/i-love-crispy-laptops/CompLing/CompLing%206%20-%20Misspells/16.01.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m all_words \u001b[39m=\u001b[39mclean_w(data)\n",
      "\u001b[1;32m/workspaces/i-love-crispy-laptops/CompLing/CompLing 6 - Misspells/16.01.ipynb Ячейка 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bpotential-eureka-6qvg95vvgqjf4xq/workspaces/i-love-crispy-laptops/CompLing/CompLing%206%20-%20Misspells/16.01.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_w\u001b[39m(text): \u001b[39m# Приводит к н.ф., запись и возращает все токены текста\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bpotential-eureka-6qvg95vvgqjf4xq/workspaces/i-love-crispy-laptops/CompLing/CompLing%206%20-%20Misspells/16.01.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     sents \u001b[39m=\u001b[39m sent_tokenize(text, language\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrussian\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bpotential-eureka-6qvg95vvgqjf4xq/workspaces/i-love-crispy-laptops/CompLing/CompLing%206%20-%20Misspells/16.01.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     cleaned_text \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bpotential-eureka-6qvg95vvgqjf4xq/workspaces/i-love-crispy-laptops/CompLing/CompLing%206%20-%20Misspells/16.01.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     all_words \u001b[39m=\u001b[39m [] \n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/nltk/tokenize/__init__.py:120\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1280\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1277\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1340\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[1;32m   1332\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1334\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_words = clean_w(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
